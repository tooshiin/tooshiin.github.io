---
title: "CacheNet: A Model Caching Framework for Deep Learning Inference on Edge"
collection: publications
permalink: /publication/2020-02-15-paper-title-number-1
excerpt: '(TBA) CacheNet is a neural network partitioning and caching scheme. It partitions a neural network into multiple smaller specialized neural networks in the cloud, where a server selects partitions for inference.'
date: 2020-06-30
venue: '(TBA)'
paperurl: '[TBA]'
citation: '[TBA]'
---

 (TBA) CacheNet is a neural network partitioning and caching scheme. It partitions a neural network into multiple smaller specialized neural networks in the cloud, where a server selects partitions for inference. The smaller specialized neural network will work well on consecutive frames over a short period. An entropy function on the input is used to decide if the cached device model needs to be updated with another model from the server. We achieve short end-to-end latency, with negligble effects on accuracy.

<!-- [Download paper here](http://academicpages.github.io/files/paper1.pdf)

Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1). -->